{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article retrieval\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_url = 'https://arstechnica.com/'\n",
    "# page_text = requests.get(pageUrl).text\n",
    "# parsed_page = BeautifulSoup(page_text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use scrapy instead\n",
    "def retrieve_parsed_page(url):\n",
    "    page_text = requests.get(url).text\n",
    "    return BeautifulSoup(page_text, 'lxml')\n",
    "\n",
    "def get_article_links(parsed_page):\n",
    "    anchor_list = parsed_page.select('h2 a')\n",
    "    return [a['href'] for a in anchor_list]\n",
    "\n",
    "def get_article_text(parsed_page):\n",
    "    page_paragraphs = parsed_page.find('section', {'class': 'article-guts'}).findAll('p')\n",
    "    paragraph_text = [paragraph.text for paragraph in page_paragraphs]\n",
    "    return '\\n'.join(paragraph_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving article: https://arstechnica.com/gadgets/2017/12/guidemaster-everything-amazons-alexa-can-do-plus-the-best-skills-to-enable/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/ars-technicas-best-video-games-of-2017/\n",
      "retrieving article: https://arstechnica.com/science/2017/12/9-himalayan-yeti-samples-turn-out-to-be-bears-dog/\n",
      "retrieving article: https://arstechnica.com/tech-policy/2017/12/library-of-congress-to-get-selective-about-the-tweets-it-keeps/\n",
      "retrieving article: https://arstechnica.com/information-technology/2017/12/mozilla-squashes-critical-thunderbird-bug/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/big-titles-must-wait-as-nintendo-pushes-back-64gb-switch-game-card-rollout/\n",
      "retrieving article: https://arstechnica.com/tech-policy/2017/12/these-experts-figured-out-why-so-many-bogus-patents-get-approved/\n",
      "retrieving article: https://arstechnica.com/gadgets/2017/12/whatsapp-to-drop-blackberry-windows-phone-8-0-support-after-new-years-eve/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/nerdy-films-in-2017-ars-picks-the-years-best-and-most-beautiful/\n",
      "retrieving article: https://arstechnica.com/gadgets/2017/12/dear-silicon-valley-a-sous-vide-is-not-a-slow-cooker/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/virtual-realitys-best-experiences-and-biggest-teleportation-steps-in-2017/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/lost-destiny-symphonic-album-complete-with-paul-mccartney-has-totally-leaked/\n",
      "retrieving article: https://arstechnica.com/science/2017/12/making-us-agriculture-sustainable-cut-beef-eating-in-half/\n",
      "retrieving article: https://arstechnica.com/cars/2017/12/driverless-cars-became-a-reality-in-2017-and-hardly-anyone-noticed/\n",
      "retrieving article: https://arstechnica.com/cars/2017/12/2017-behind-the-wheel-our-favorite-cars-of-the-year/\n",
      "retrieving article: https://arstechnica.com/science/2017/12/apollo-triumph/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/10-excellent-comics-that-flew-under-the-radar-in-2017/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/a-christmas-gift-from-game-boy-rom-hackers-super-mario-land-2-in-color/\n",
      "retrieving article: https://arstechnica.com/science/2017/12/where-did-mars-water-go-maybe-into-the-planets-interior/\n",
      "retrieving article: https://arstechnica.com/staff/2017/12/the-most-talked-about-stories-on-ars-technica-in-2017/\n",
      "retrieving article: https://arstechnica.com/gadgets/2017/12/our-favorite-and-least-favorite-tech-of-2017/\n",
      "retrieving article: https://arstechnica.com/staff/2017/12/the-most-popular-stories-of-2017/\n",
      "retrieving article: https://arstechnica.com/science/2017/12/here-are-the-intriguing-toxins-that-spice-up-our-favorite-holiday-dishes/\n",
      "retrieving article: https://arstechnica.com/gaming/2017/12/the-year-in-ars-techni-cky-tv-from-sci-fi-reboots-to-social-media-horror/\n",
      "retrieving article: https://arstechnica.com/tech-policy/2017/12/how-do-you-change-the-most-important-law-in-internet-history-carefully/\n",
      "retrieving article: https://arstechnica.com/science/2017/12/study-says-mantle-motion-not-a-plume-or-hot-spot-made-americas-volcanoes/\n",
      "retrieving article: https://arstechnica.com/information-technology/2017/12/okcupid-begins-enforcing-real-name-rules-insists-its-a-good-idea/\n",
      "retrieving article: https://arstechnica.com/gadgets/2017/12/parents-cant-use-the-iphone-xs-face-id-to-approve-family-purchases/\n",
      "retrieving article: https://arstechnica.com/tech-policy/2017/12/days-after-iphone-battery-fiasco-lawsuits-against-apple-begin-to-mount/\n",
      "retrieving article: https://arstechnica.com/staff/2017/12/dealmaster-get-a-100-itunes-gift-card-for-85/\n",
      "retrieving article: https://arstechnica.com/gadgets/2017/12/nvidia-to-cease-producing-new-drivers-for-32-bit-systems/\n",
      "done parsing 31 articles\n"
     ]
    }
   ],
   "source": [
    "parsed_homepage = retrieve_parsed_page(home_url)\n",
    "article_urls = get_article_links(parsed_homepage)\n",
    "\n",
    "article_list = []\n",
    "for url in article_urls:\n",
    "    print(f'retrieving article: {url}')\n",
    "    parsed_page = retrieve_parsed_page(url)\n",
    "    article_text = get_article_text(parsed_page)\n",
    "    article_list.append(article_text)\n",
    "print(f'done parsing {len(article_list)} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_instance = TfidfVectorizer(\n",
    "    max_df=0.65, # exclude words that appear in more than 50% of the corpus\n",
    "    min_df=2, # exclude words that appear less than 2 times total. (cut )\n",
    "    stop_words='english', \n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_v = CountVectorizer()\n",
    "# print(count_v.fit_transform(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer_instance.fit_transform(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31x1619 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5482 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(X[0].toarray())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_fitter = KMeans(\n",
    "    n_clusters = 3,\n",
    "    init='k-means++',\n",
    "    max_iter=100,\n",
    "    n_init=1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 48.582\n",
      "Iteration  1, inertia 25.441\n",
      "Converged at iteration 1: center shift 0.000000e+00 within tolerance 5.606854e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_fitter.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2], dtype=int32), array([ 6, 19,  6]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(km_fitter.labels_, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_articles = {}\n",
    "for group, article in zip(km_fitter.labels_, article_list):\n",
    "    try:\n",
    "         grouped_articles[group].append(article)\n",
    "    except KeyError:\n",
    "        grouped_articles[group] = [article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english') + list(punctuation) + ['’', '“', '”', '\\'s']\n",
    "newline_char = '\\n'\n",
    "\n",
    "def tokenize_words(text):\n",
    "    tokenized_words = word_tokenize(text.lower())\n",
    "    return [word for word in tokenized_words if word not in stopword_list]\n",
    "\n",
    "def top_occurring_words(word_list, top_n=10):\n",
    "    word_frequency = FreqDist(word_list)\n",
    "    top_words = nlargest(top_n, word_frequency, key=word_frequency.get)\n",
    "    return [(word, word_frequency[word]) for word in top_words]\n",
    "\n",
    "def get_exclusion_set(group_name, grouped_articles):\n",
    "    other_articles = '\\n'.join(\n",
    "        '\\n'.join(articles)\n",
    "        for group, articles\n",
    "        in grouped_articles.items()\n",
    "        if group != group_name\n",
    "    )\n",
    "    return set(tokenize_words(other_articles))\n",
    "\n",
    "def get_word_list(articles, group_name, grouped_articles):\n",
    "    merged_list = '\\n'.join(article_list)\n",
    "    tokenized_words = tokenize_words(merged_list)\n",
    "    exclusion_set = get_exclusion_set(group_name, grouped_articles)\n",
    "    return [\n",
    "        word\n",
    "        for word\n",
    "        in tokenized_words\n",
    "        if word not in exclusion_set\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_article_list(articles, group_name, grouped_articles, top_n=40):\n",
    "    word_list = get_word_list(articles, group_name, grouped_articles)\n",
    "    return top_occurring_words(word_list, top_n)\n",
    "\n",
    "    \n",
    "def display_top_unique_words_by_group(grouped_articles, n_words):\n",
    "    \"\"\"\n",
    "    Display the n most used words unique to each group (i.e. not used in any other group)\n",
    "    \"\"\"\n",
    "    merged_articles = {\n",
    "        group: parse_article_list(articles, group, grouped_articles, n_words)\n",
    "        for group, articles \n",
    "        in grouped_articles.items()\n",
    "    }\n",
    "    \n",
    "    for group, word_list in merged_articles.items():\n",
    "        tuple_joiner = lambda tup: f'{tup[0]} - {tup[1]}'\n",
    "        print('{group}: {word}\\n'.format(\n",
    "            group=group,\n",
    "            word=' | '.join((tuple_joiner(word) for word in word_list))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: alexa - 50 | apple - 37 | patent - 32 | waymo - 32 | cars - 28 | 230 - 24 | driverless - 23 | iphone - 18 | id - 18 | patents - 16\n",
      "\n",
      "0: vr - 24 | lunar - 24 | module - 18 | destiny - 12 | nintendo - 12 | bungie - 11 | o'donnell - 9 | orbit - 9 | storage - 7 | htc - 7\n",
      "\n",
      "2: sous-vide - 32 | water - 27 | food - 25 | mellow - 18 | mantle - 18 | rock - 13 | fda - 13 | mars - 12 | beef - 11 | eating - 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_top_unique_words_by_group(grouped_articles, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
